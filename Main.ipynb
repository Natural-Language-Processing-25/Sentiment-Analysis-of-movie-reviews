{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1595eb27",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1aabd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.3 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: spacy==3.7.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 3)) (3.7.2)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 4)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: matplotlib==3.8.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: seaborn==0.13.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 7)) (0.13.0)\n",
      "Requirement already satisfied: ipython==8.18.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 8)) (8.18.1)\n",
      "Requirement already satisfied: datasets==2.16.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 9)) (2.16.1)\n",
      "Requirement already satisfied: transformers==4.36.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 10)) (4.36.2)\n",
      "Requirement already satisfied: accelerate==0.25.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 11)) (0.25.0)\n",
      "Requirement already satisfied: torch==2.1.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 13)) (2.1.2)\n",
      "Requirement already satisfied: thinc<8.3.6 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 14)) (8.2.5)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from -r requirements.txt (line 12)) (0.31.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (8.2.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (2024.11.6)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 5)) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (2.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1->-r requirements.txt (line 9)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (3.11.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from transformers==4.36.2->-r requirements.txt (line 10)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from transformers==4.36.2->-r requirements.txt (line 10)) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from accelerate==0.25.0->-r requirements.txt (line 11)) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from torch==2.1.2->-r requirements.txt (line 13)) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from torch==2.1.2->-r requirements.txt (line 13)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from torch==2.1.2->-r requirements.txt (line 13)) (3.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from thinc<8.3.6->-r requirements.txt (line 14)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from thinc<8.3.6->-r requirements.txt (line 14)) (0.1.5)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython==8.18.1->-r requirements.txt (line 8)) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 3)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (2025.4.26)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from huggingface_hub[hf_xet]->-r requirements.txt (line 12)) (1.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (1.20.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from jedi>=0.16->ipython==8.18.1->-r requirements.txt (line 8)) (0.8.4)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from jinja2->spacy==3.7.2->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from stack-data->ipython==8.18.1->-r requirements.txt (line 8)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from stack-data->ipython==8.18.1->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from stack-data->ipython==8.18.1->-r requirements.txt (line 8)) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hanee\\anaconda3\\envs\\sentiment-nlp\\lib\\site-packages (from sympy->torch==2.1.2->-r requirements.txt (line 13)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# # You may need to run those in your enviroment terminal.\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6641483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hanee\\anaconda3\\envs\\sentiment-nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hanee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hanee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a5438",
   "metadata": {},
   "source": [
    "Creating data frame of the data and assigning them labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d524bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>if anything , \" stigmata \" should be taken as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>john boorman's \" zardoz \" is a goofy cinematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>the kids in the hall are an acquired taste . \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>two party guys bob their heads to haddaway's d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1995      0  if anything , \" stigmata \" should be taken as ...\n",
       "1996      0  john boorman's \" zardoz \" is a goofy cinematic...\n",
       "1997      0  the kids in the hall are an acquired taste . \\...\n",
       "1998      0  there was a time when john carpenter was a gre...\n",
       "1999      0  two party guys bob their heads to haddaway's d..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "base_path = Path('review_polarity/txt_sentoken')\n",
    "pos_path = base_path / 'pos'\n",
    "neg_path = base_path / 'neg'\n",
    "\n",
    "# Assign label 1\n",
    "if pos_path.exists():\n",
    "    for file in pos_path.glob('*.txt'):\n",
    "        with open(file, 'r', encoding = 'utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)\n",
    "\n",
    "# Assign label 0\n",
    "if neg_path.exists():\n",
    "    for file in neg_path.glob('*.txt'):\n",
    "        with open(file, 'r', encoding = 'utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'label': labels,\n",
    "    'text': texts\n",
    "})\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad59ad76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1716a",
   "metadata": {},
   "source": [
    "No duplicates were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d84703-7454-4dd4-b1c8-9a4d5b6d5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_to_wordnet_pos(spacy_pos):\n",
    "    if spacy_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif spacy_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif spacy_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif spacy_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f35a43-5c47-49ad-8cc6-95ffa03211cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    synonyms.discard(word)      # Remove the original word to avoid replacement with itself\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161f45fe-6246-4ff3-895c-85d4092ca01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_text_with_synonyms(tokens, pos_tags, synonym_probability = 0.2):\n",
    "    augmented_tokens = []\n",
    "\n",
    "    for token, pos_tag in zip(tokens, pos_tags):\n",
    "        if random.random() < synonym_probability:\n",
    "            if pos_tag in ['n', 'v', 'a']:   #nouns adjectives and verbs\n",
    "                synonyms = get_synonyms(token)\n",
    "                if synonyms:\n",
    "                    new_word = random.choice(synonyms)\n",
    "                    augmented_tokens.append(new_word)\n",
    "                    continue\n",
    "        augmented_tokens.append(token)         # Add the original word if no augmentation is done\n",
    "\n",
    "    return augmented_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33efe070-6a00-4518-b544-7ee483699d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation_to_dataset(word_tokens, pos_tags, texts, labels, sentence_tokens, synonym_probability = 0.2):\n",
    "    all_word_tokens = word_tokens.copy()\n",
    "    all_texts = texts.copy()  # Original text is preserved\n",
    "    all_labels = labels.copy()\n",
    "    all_sentence_tokens = sentence_tokens.copy()\n",
    "    all_pos_tags = pos_tags.copy()\n",
    "\n",
    "    for tokens, pos, label, sentence, text in zip(word_tokens, pos_tags, labels, sentence_tokens, texts):\n",
    "        augmented_tokens = augment_text_with_synonyms(tokens, pos, synonym_probability)\n",
    "\n",
    "        all_word_tokens.append(augmented_tokens)\n",
    "        all_texts.append(text)  # Keep the original text\n",
    "        all_labels.append(label)\n",
    "        all_pos_tags.append(pos)\n",
    "        all_sentence_tokens.append(sentence)  # Sentence tokens are not augmented\n",
    "\n",
    "    return all_word_tokens, all_pos_tags, all_texts, all_labels, all_sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4aac42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(tokens):\n",
    "    return [token.lemma_ for token in nlp(' '.join(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d4bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(text):\n",
    "\n",
    "    words = text.split()\n",
    "    return ' '.join([PorterStemmer().stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tf_idf_heatmap(tfidf_matrix, feature_names, n_top_features = 20, n_top_docs = 10):\n",
    "    # Get the top features by summing TF-IDF scores across documents\n",
    "    tfidf_array = tfidf_matrix.toarray()\n",
    "    feature_importance = np.sum(tfidf_array, axis = 0)\n",
    "    top_feature_indices = np.argsort(feature_importance)[-n_top_features:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_feature_indices]\n",
    "\n",
    "    # Get the top documents by summing TF-IDF scores across features\n",
    "    doc_importance = np.sum(tfidf_array, axis=1)\n",
    "    top_doc_indices = np.argsort(doc_importance)[-n_top_docs:][::-1]\n",
    "\n",
    "    # Extract the submatrix for visualization\n",
    "    sub_matrix = tfidf_array[np.ix_(top_doc_indices, top_feature_indices)]\n",
    "\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    sns.heatmap(\n",
    "        sub_matrix,\n",
    "        annot = True,          # Show values in cells\n",
    "        fmt = '.3f',           # Format with 3 decimal places\n",
    "        cmap = 'YlGnBu',       # Better colormap\n",
    "        xticklabels = top_features,\n",
    "        yticklabels = range(n_top_docs)\n",
    "    )\n",
    "    plt.title(f'TF-IDF Heatmap (Top {n_top_features} Features, First {n_top_docs} Documents)')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Documents')\n",
    "    plt.xticks(rotation = 45, ha = 'right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tfidf_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Bar chart of top features across the corpus\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_features_scores = [feature_importance[i] for i in top_feature_indices]\n",
    "    plt.bar(top_features, top_features_scores)\n",
    "    plt.title('Top TF-IDF Features Across All Documents')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Sum of TF-IDF Scores')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tfidf_top_features.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bd110ab-6e79-481b-9108-ac484b4acd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix Shape: (4000, 42868)\n",
      "Number of features: 42868\n"
     ]
    }
   ],
   "source": [
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "word_tokens = []\n",
    "sentence_tokens = []\n",
    "pos_tags = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    sentence_tokens.append([sent.text.strip() for sent in doc.sents])\n",
    "    tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_space]\n",
    "    word_tokens.append([token.text.lower() for token in tokens])\n",
    "    pos_tags.append([spacy_pos_to_wordnet_pos(token.tag_) for token in tokens])\n",
    "\n",
    "lemmatized_tokens = [apply_lemmatization(t) for t in word_tokens]\n",
    "\n",
    "word_tokens, pos_tags, texts, labels, sentence_tokens = apply_augmentation_to_dataset(\n",
    "    lemmatized_tokens, pos_tags, texts, labels, sentence_tokens\n",
    ")\n",
    "\n",
    "joined_texts = [' '.join(tokens) for tokens in word_tokens]\n",
    "\n",
    "augmented_df = pd.DataFrame({\n",
    "    'label': labels,\n",
    "    'text': joined_texts,\n",
    "    'word_tokens': word_tokens\n",
    "})\n",
    "\n",
    "augmented_df = augmented_df.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    augmented_df['text'], augmented_df['label'], test_size = 0.2, random_state = 42\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_texts)\n",
    "xtrain_tfidf = vectorizer.transform(train_texts)\n",
    "xtest_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(joined_texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nTF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "visualize_tf_idf_heatmap(tfidf_matrix, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05795c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented_df.head():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>harmless silly fun comedy dim witte wrestle fa...</td>\n",
       "      <td>[harmless, silly, fun, comedy, dim, witte, wre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>point movie staging opera go completely wrong ...</td>\n",
       "      <td>[point, movie, staging, opera, go, completely,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>sick life death bob flanagan supermasochist fe...</td>\n",
       "      <td>[sick, life, death, bob, flanagan, supermasoch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>everybody film think alicia documentary see cr...</td>\n",
       "      <td>[everybody, film, think, alicia, documentary, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>lisa cholodenko heights art intelligent quiet ...</td>\n",
       "      <td>[lisa, cholodenko, heights, art, intelligent, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  harmless silly fun comedy dim witte wrestle fa...   \n",
       "1      0  point movie staging opera go completely wrong ...   \n",
       "2      1  sick life death bob flanagan supermasochist fe...   \n",
       "3      0  everybody film think alicia documentary see cr...   \n",
       "4      1  lisa cholodenko heights art intelligent quiet ...   \n",
       "\n",
       "                                         word_tokens  \n",
       "0  [harmless, silly, fun, comedy, dim, witte, wre...  \n",
       "1  [point, movie, staging, opera, go, completely,...  \n",
       "2  [sick, life, death, bob, flanagan, supermasoch...  \n",
       "3  [everybody, film, think, alicia, documentary, ...  \n",
       "4  [lisa, cholodenko, heights, art, intelligent, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df.head():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>you've got mail works alot better than it dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>\" jaws \" is a rare film that grabs your atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  films adapted from comic books have had plenty...\n",
       "1      1  every now and then a movie comes along from a ...\n",
       "2      1  you've got mail works alot better than it dese...\n",
       "3      1   \" jaws \" is a rare film that grabs your atten...\n",
       "4      1  moviemaking is a lot like being the general ma..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"augmented_df.head():\")\n",
    "display(augmented_df.head())\n",
    "\n",
    "print(\"\\ndf.head():\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe295e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented_df.tail():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0</td>\n",
       "      <td>understand clich hell earth truly mean recentl...</td>\n",
       "      <td>[understand, clich, hell, earth, truly, mean, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0</td>\n",
       "      <td>1954 japanese monster film godzilla transform ...</td>\n",
       "      <td>[1954, japanese, monster, film, godzilla, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1</td>\n",
       "      <td>verdict spine chilling drama horror maestro st...</td>\n",
       "      <td>[verdict, spine, chilling, drama, horror, maes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0</td>\n",
       "      <td>midway anaconda documentary filmmaker terri fl...</td>\n",
       "      <td>[midway, anaconda, documentary, filmmaker, ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0</td>\n",
       "      <td>starship trooper regretful moving_picture mean...</td>\n",
       "      <td>[starship, trooper, regretful, moving_picture,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "3995      0  understand clich hell earth truly mean recentl...   \n",
       "3996      0  1954 japanese monster film godzilla transform ...   \n",
       "3997      1  verdict spine chilling drama horror maestro st...   \n",
       "3998      0  midway anaconda documentary filmmaker terri fl...   \n",
       "3999      0  starship trooper regretful moving_picture mean...   \n",
       "\n",
       "                                            word_tokens  \n",
       "3995  [understand, clich, hell, earth, truly, mean, ...  \n",
       "3996  [1954, japanese, monster, film, godzilla, tran...  \n",
       "3997  [verdict, spine, chilling, drama, horror, maes...  \n",
       "3998  [midway, anaconda, documentary, filmmaker, ter...  \n",
       "3999  [starship, trooper, regretful, moving_picture,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df.tail():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>if anything , \" stigmata \" should be taken as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>john boorman's \" zardoz \" is a goofy cinematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>the kids in the hall are an acquired taste . \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>two party guys bob their heads to haddaway's d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1995      0  if anything , \" stigmata \" should be taken as ...\n",
       "1996      0  john boorman's \" zardoz \" is a goofy cinematic...\n",
       "1997      0  the kids in the hall are an acquired taste . \\...\n",
       "1998      0  there was a time when john carpenter was a gre...\n",
       "1999      0  two party guys bob their heads to haddaway's d..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"augmented_df.tail():\")\n",
    "display(augmented_df.tail())\n",
    "\n",
    "print(\"\\ndf.tail():\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "039b47bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df dimentions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "augmented_df dimentions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\ndf dimentions:\")\n",
    "display(df.shape)\n",
    "\n",
    "print(\"\\naugmented_df dimentions:\")\n",
    "display(augmented_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67965868",
   "metadata": {},
   "source": [
    "# Modelling - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a699ea3",
   "metadata": {},
   "source": [
    "####        â€¢    ML -> Logistic Regression, Naive Bayes, SVM, Decision tree, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f41270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, train_labels, feature_vector_test, test_labels, is_neural_net=False):\n",
    "    classifier.fit(feature_vector_train, train_labels)\n",
    "\n",
    "    train_predictions = classifier.predict(feature_vector_train)\n",
    "    test_predictions = classifier.predict(feature_vector_test)\n",
    "    train_accuracy = metrics.accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = metrics.accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9d515",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2711bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.06%\n",
      "Test Accuracy: 91.38%\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_labels, xtest_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00af1b",
   "metadata": {},
   "source": [
    "#### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6801ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.94%\n",
      "Test Accuracy: 92.12%\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha = 0.0001), xtrain_tfidf, train_labels, xtest_tfidf,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1b670",
   "metadata": {},
   "source": [
    "#### 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e74a87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.62%\n",
      "Test Accuracy: 95.50%\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(kernel = 'linear', C = 1.0), xtrain_tfidf, train_labels, xtest_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9d0071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.94%\n",
      "Test Accuracy: 96.38%\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(kernel = 'rbf', C = 1.0), xtrain_tfidf, train_labels, xtest_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a3eefc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 92.88%\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(kernel = 'poly', C = 1.0, degree = 3), xtrain_tfidf, train_labels, xtest_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e209c",
   "metadata": {},
   "source": [
    "#### 4. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "067e770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 79.75%\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(DecisionTreeClassifier(), xtrain_tfidf, train_labels, xtest_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7281ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 94.88%\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(RandomForestClassifier(), xtrain_tfidf, train_labels, xtest_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe38d91",
   "metadata": {},
   "source": [
    "#### â€¢ DL -> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ba3d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use same train/test split for BERT\n",
    "bert_train_texts = train_texts.tolist()\n",
    "bert_test_texts = test_texts.tolist()\n",
    "bert_train_labels = train_labels.tolist()\n",
    "bert_test_labels = test_labels.tolist()\n",
    "\n",
    "bert_train_ds = Dataset.from_dict({\"text\": bert_train_texts, \"label\": bert_train_labels})\n",
    "bert_test_ds = Dataset.from_dict({\"text\": bert_test_texts, \"label\": bert_test_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21bd2802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3200/3200 [00:14<00:00, 223.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:04<00:00, 181.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "bert_train_ds = bert_train_ds.map(tokenize_function, batched=True)\n",
    "bert_test_ds = bert_test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set PyTorch format\n",
    "bert_train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "bert_test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "addefa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hanee\\anaconda3\\envs\\sentiment-nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hanee\\anaconda3\\envs\\sentiment-nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hanee\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained BERT for classification\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define evaluation metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8aadf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [52:24<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6949, 'learning_rate': 4.96875e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [52:34<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7136, 'learning_rate': 4.937500000000001e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [52:43<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6651, 'learning_rate': 4.90625e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [52:53<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6833, 'learning_rate': 4.875e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [53:02<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6821, 'learning_rate': 4.8437500000000005e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [53:12<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7037, 'learning_rate': 4.8125000000000004e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [53:23<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6878, 'learning_rate': 4.7812500000000003e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [53:35<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6995, 'learning_rate': 4.75e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [53:46<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7071, 'learning_rate': 4.71875e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [53:58<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.699, 'learning_rate': 4.6875e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [54:10<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.679, 'learning_rate': 4.65625e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [54:21<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6936, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [54:33<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6877, 'learning_rate': 4.59375e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [54:44<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6793, 'learning_rate': 4.5625e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [54:57<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.709, 'learning_rate': 4.5312500000000004e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [55:08<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7628, 'learning_rate': 4.5e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [55:20<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.674, 'learning_rate': 4.46875e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [55:31<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6997, 'learning_rate': 4.4375e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [55:41<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.717, 'learning_rate': 4.40625e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [55:52<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7824, 'learning_rate': 4.375e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [56:04<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6403, 'learning_rate': 4.3437500000000006e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [56:15<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7374, 'learning_rate': 4.3125000000000005e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [56:27<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6881, 'learning_rate': 4.28125e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [56:39<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6862, 'learning_rate': 4.25e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [56:51<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5871, 'learning_rate': 4.21875e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [57:02<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6859, 'learning_rate': 4.1875e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [57:14<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7591, 'learning_rate': 4.156250000000001e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [57:26<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7206, 'learning_rate': 4.125e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [57:37<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7676, 'learning_rate': 4.09375e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [57:47<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6201, 'learning_rate': 4.0625000000000005e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [57:56<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6127, 'learning_rate': 4.0312500000000004e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [58:06<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6794, 'learning_rate': 4e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [58:16<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5358, 'learning_rate': 3.96875e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [58:26<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5371, 'learning_rate': 3.9375e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [58:38<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6883, 'learning_rate': 3.90625e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [58:50<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5609, 'learning_rate': 3.875e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [59:01<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6352, 'learning_rate': 3.8437500000000006e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [59:10<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4498, 'learning_rate': 3.8125e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [59:20<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7485, 'learning_rate': 3.78125e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [59:30<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.002, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [59:39<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6762, 'learning_rate': 3.71875e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [59:48<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6513, 'learning_rate': 3.6875e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [59:57<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6114, 'learning_rate': 3.65625e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:00:07<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6087, 'learning_rate': 3.625e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:00:16<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6329, 'learning_rate': 3.59375e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:00:25<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6015, 'learning_rate': 3.5625000000000005e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:00:35<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5338, 'learning_rate': 3.5312500000000005e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:00:46<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6918, 'learning_rate': 3.5e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:00:55<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5852, 'learning_rate': 3.46875e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:01:05<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6365, 'learning_rate': 3.4375e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:01:15<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6172, 'learning_rate': 3.40625e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:01:25<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5225, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:01:35<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4876, 'learning_rate': 3.34375e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:01:45<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5132, 'learning_rate': 3.3125e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:01:55<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5585, 'learning_rate': 3.2812500000000005e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:02:04<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0942, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:02:13<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5168, 'learning_rate': 3.21875e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:02:22<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6261, 'learning_rate': 3.1875e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:02:31<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5373, 'learning_rate': 3.15625e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:02:41<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.734, 'learning_rate': 3.125e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:02:51<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9185, 'learning_rate': 3.09375e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:03:03<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8238, 'learning_rate': 3.0625000000000006e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:03:15<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5832, 'learning_rate': 3.0312499999999998e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:03:26<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.676, 'learning_rate': 3e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:03:37<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4524, 'learning_rate': 2.96875e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:03:49<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5444, 'learning_rate': 2.9375000000000003e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:04:00<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4986, 'learning_rate': 2.9062500000000005e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:04:12<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6724, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:04:24<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.264, 'learning_rate': 2.84375e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:04:35<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4163, 'learning_rate': 2.8125000000000003e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:04:47<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8597, 'learning_rate': 2.7812500000000002e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:04:59<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5544, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:05:10<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5256, 'learning_rate': 2.71875e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:05:22<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.719, 'learning_rate': 2.6875e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:05:33<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3417, 'learning_rate': 2.6562500000000002e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:05:44<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5647, 'learning_rate': 2.625e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:05:56<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6001, 'learning_rate': 2.5937500000000004e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:06:07<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5421, 'learning_rate': 2.5625e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:06:19<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4268, 'learning_rate': 2.53125e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:06:29<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9622, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:07:19<1:28:46,  7.88s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6925792098045349, 'eval_accuracy': 0.77875, 'eval_runtime': 50.1062, 'eval_samples_per_second': 15.966, 'eval_steps_per_second': 3.992, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:07:28<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1866, 'learning_rate': 2.4687500000000004e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:07:37<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2208, 'learning_rate': 2.4375e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:07:46<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6386, 'learning_rate': 2.4062500000000002e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:07:56<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5651, 'learning_rate': 2.375e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:08:05<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6691, 'learning_rate': 2.34375e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:08:14<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3993, 'learning_rate': 2.3125000000000003e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:08:23<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4672, 'learning_rate': 2.28125e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:08:32<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.605, 'learning_rate': 2.25e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:08:41<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3332, 'learning_rate': 2.21875e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:08:51<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3247, 'learning_rate': 2.1875e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:09:00<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.153, 'learning_rate': 2.1562500000000002e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:09:08<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5182, 'learning_rate': 2.125e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:09:18<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8589, 'learning_rate': 2.09375e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:09:30<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.405, 'learning_rate': 2.0625e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:09:42<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6322, 'learning_rate': 2.0312500000000002e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:09:54<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7311, 'learning_rate': 2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:10:06<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5217, 'learning_rate': 1.96875e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:10:17<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2812, 'learning_rate': 1.9375e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:10:29<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3504, 'learning_rate': 1.90625e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:10:41<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2746, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:10:53<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1044, 'learning_rate': 1.84375e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:11:05<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4267, 'learning_rate': 1.8125e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:11:17<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3247, 'learning_rate': 1.7812500000000003e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:11:29<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.475, 'learning_rate': 1.75e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:11:41<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6243, 'learning_rate': 1.71875e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:11:53<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.327, 'learning_rate': 1.6875000000000004e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:12:05<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.235, 'learning_rate': 1.65625e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:12:17<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3182, 'learning_rate': 1.6250000000000002e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:12:29<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3631, 'learning_rate': 1.59375e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:12:40<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3104, 'learning_rate': 1.5625e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:12:52<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5867, 'learning_rate': 1.5312500000000003e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:13:04<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4737, 'learning_rate': 1.5e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:13:16<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.257, 'learning_rate': 1.4687500000000001e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:13:28<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3026, 'learning_rate': 1.4374999999999999e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:13:40<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0586, 'learning_rate': 1.4062500000000001e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:13:52<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.242, 'learning_rate': 1.3750000000000002e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:14:03<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5873, 'learning_rate': 1.34375e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:14:16<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4923, 'learning_rate': 1.3125e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:14:27<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3872, 'learning_rate': 1.28125e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:14:39<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6532, 'learning_rate': 1.25e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:14:51<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2661, 'learning_rate': 1.21875e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:15:03<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4156, 'learning_rate': 1.1875e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:15:15<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3807, 'learning_rate': 1.1562500000000002e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:15:27<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1825, 'learning_rate': 1.125e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:15:38<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2172, 'learning_rate': 1.09375e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:15:50<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.209, 'learning_rate': 1.0625e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:16:02<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4066, 'learning_rate': 1.03125e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:16:14<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6688, 'learning_rate': 1e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:16:26<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3604, 'learning_rate': 9.6875e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:16:37<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1918, 'learning_rate': 9.375000000000001e-06, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:16:49<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2957, 'learning_rate': 9.0625e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:17:01<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1856, 'learning_rate': 8.75e-06, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:17:13<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4526, 'learning_rate': 8.437500000000002e-06, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:17:25<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4037, 'learning_rate': 8.125000000000001e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:17:37<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.44, 'learning_rate': 7.8125e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:17:49<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5121, 'learning_rate': 7.5e-06, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:18:00<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1664, 'learning_rate': 7.187499999999999e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:18:12<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2296, 'learning_rate': 6.875000000000001e-06, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:18:24<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2952, 'learning_rate': 6.5625e-06, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:18:36<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3449, 'learning_rate': 6.25e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:18:47<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4293, 'learning_rate': 5.9375e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:18:59<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1872, 'learning_rate': 5.625e-06, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:19:11<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4643, 'learning_rate': 5.3125e-06, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:19:22<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6485, 'learning_rate': 5e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:19:34<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1159, 'learning_rate': 4.6875000000000004e-06, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:19:46<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3913, 'learning_rate': 4.375e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:19:58<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1918, 'learning_rate': 4.0625000000000005e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:20:07<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2998, 'learning_rate': 3.75e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:20:16<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1857, 'learning_rate': 3.4375000000000005e-06, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:20:25<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2649, 'learning_rate': 3.125e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:20:34<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6797, 'learning_rate': 2.8125e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:20:46<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3305, 'learning_rate': 2.5e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:20:58<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4049, 'learning_rate': 2.1875e-06, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:21:10<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3921, 'learning_rate': 1.875e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:21:22<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2064, 'learning_rate': 1.5625e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:21:34<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5078, 'learning_rate': 1.25e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:21:45<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3842, 'learning_rate': 9.375e-07, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:21:57<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3559, 'learning_rate': 6.25e-07, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:22:09<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3508, 'learning_rate': 3.125e-07, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:22:21<1:28:46,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3122, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      " 16%|â–ˆâ–Œ        | 124/800 [1:23:22<1:28:46,  7.88s/it]\n",
      "\u001b[A\n",
      "                                                     \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [31:06<00:00,  1.17s/it]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5407305359840393, 'eval_accuracy': 0.845, 'eval_runtime': 61.0636, 'eval_samples_per_second': 13.101, 'eval_steps_per_second': 3.275, 'epoch': 2.0}\n",
      "{'train_runtime': 1866.4234, 'train_samples_per_second': 3.429, 'train_steps_per_second': 0.857, 'train_loss': 0.5134868958964944, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1600, training_loss=0.5134868958964944, metrics={'train_runtime': 1866.4234, 'train_samples_per_second': 3.429, 'train_steps_per_second': 0.857, 'train_loss': 0.5134868958964944, 'epoch': 2.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_train_ds,\n",
    "    eval_dataset=bert_test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "626ffc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:48<00:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classification Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BERT model on test set\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"BERT Classification Accuracy: {metrics['eval_accuracy']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
