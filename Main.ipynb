{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1595eb27",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1aabd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3 (from -r requirements.txt (line 1))\n",
      "  Using cached numpy-1.24.3.tar.gz (10.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"E:\\anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"E:\\anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"E:\\anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "      backend = _build_backend()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "    File \"E:\\anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "      obj = import_module(mod_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"E:\\anaconda\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "      return _bootstrap._gcd_import(name[level:], package, level)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "    File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"C:\\Users\\menna\\AppData\\Local\\Temp\\pip-build-env-01eu9fsv\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "      import setuptools.version\n",
      "    File \"C:\\Users\\menna\\AppData\\Local\\Temp\\pip-build-env-01eu9fsv\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "      import pkg_resources\n",
      "    File \"C:\\Users\\menna\\AppData\\Local\\Temp\\pip-build-env-01eu9fsv\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "      register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# # You may need to run those in your enviroment terminal.\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6641483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a5438",
   "metadata": {},
   "source": [
    "Creating data frame of the data and assigning them labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d524bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "base_path = Path('review_polarity/txt_sentoken')\n",
    "pos_path = base_path / 'pos'\n",
    "neg_path = base_path / 'neg'\n",
    "\n",
    "# Assign label 1\n",
    "if pos_path.exists():\n",
    "    for file in pos_path.glob('*.txt'):\n",
    "        with open(file, 'r', encoding = 'utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)\n",
    "\n",
    "# Assign label 0\n",
    "if neg_path.exists():\n",
    "    for file in neg_path.glob('*.txt'):\n",
    "        with open(file, 'r', encoding = 'utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'label': labels,\n",
    "    'text': texts\n",
    "})\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1716a",
   "metadata": {},
   "source": [
    "No duplicates were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d84703-7454-4dd4-b1c8-9a4d5b6d5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_to_wordnet_pos(spacy_pos):\n",
    "    if spacy_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif spacy_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif spacy_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif spacy_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f35a43-5c47-49ad-8cc6-95ffa03211cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    synonyms.discard(word)      # Remove the original word to avoid replacement with itself\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f45fe-6246-4ff3-895c-85d4092ca01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_text_with_synonyms(tokens, pos_tags, synonym_probability = 0.2):\n",
    "    augmented_tokens = []\n",
    "\n",
    "    for token, pos_tag in zip(tokens, pos_tags):\n",
    "        if random.random() < synonym_probability:\n",
    "            if pos_tag in ['n', 'v', 'a']:   #nouns adjectives and verbs\n",
    "                synonyms = get_synonyms(token)\n",
    "                if synonyms:\n",
    "                    new_word = random.choice(synonyms)\n",
    "                    augmented_tokens.append(new_word)\n",
    "                    continue\n",
    "        augmented_tokens.append(token)         # Add the original word if no augmentation is done\n",
    "\n",
    "    return augmented_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efe070-6a00-4518-b544-7ee483699d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation_to_dataset(word_tokens, pos_tags, texts, labels, sentence_tokens, synonym_probability = 0.2):\n",
    "    all_word_tokens = word_tokens.copy()\n",
    "    all_texts = texts.copy()  # Original text is preserved\n",
    "    all_labels = labels.copy()\n",
    "    all_sentence_tokens = sentence_tokens.copy()\n",
    "    all_pos_tags = pos_tags.copy()\n",
    "\n",
    "    for tokens, pos, label, sentence, text in zip(word_tokens, pos_tags, labels, sentence_tokens, texts):\n",
    "        augmented_tokens = augment_text_with_synonyms(tokens, pos, synonym_probability)\n",
    "\n",
    "        all_word_tokens.append(augmented_tokens)\n",
    "        all_texts.append(text)  # Keep the original text\n",
    "        all_labels.append(label)\n",
    "        all_pos_tags.append(pos)\n",
    "        all_sentence_tokens.append(sentence)  # Sentence tokens are not augmented\n",
    "\n",
    "    return all_word_tokens, all_pos_tags, all_texts, all_labels, all_sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aac42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(tokens):\n",
    "    return [token.lemma_ for token in nlp(' '.join(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([PorterStemmer().stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tf_idf_heatmap(tfidf_matrix, feature_names, n_top_features = 20, n_top_docs = 10):\n",
    "    # Get the top features by summing TF-IDF scores across documents\n",
    "    tfidf_array = tfidf_matrix.toarray()\n",
    "    feature_importance = np.sum(tfidf_array, axis = 0)\n",
    "    top_feature_indices = np.argsort(feature_importance)[-n_top_features:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_feature_indices]\n",
    "\n",
    "    # Get the top documents by summing TF-IDF scores across features\n",
    "    doc_importance = np.sum(tfidf_array, axis = 1)\n",
    "    top_doc_indices = np.argsort(doc_importance)[-n_top_docs:][::-1]\n",
    "\n",
    "    # Extract the submatrix for visualization\n",
    "    sub_matrix = tfidf_array[np.ix_(top_doc_indices, top_feature_indices)]\n",
    "\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    sns.heatmap(\n",
    "        sub_matrix,\n",
    "        annot = True,          # Show values in cells\n",
    "        fmt = '.3f',           # Format with 3 decimal places\n",
    "        cmap = 'YlGnBu',       # Better colormap\n",
    "        xticklabels = top_features,\n",
    "        yticklabels = range(n_top_docs)\n",
    "    )\n",
    "    plt.title(f'TF-IDF Heatmap (Top {n_top_features} Features, First {n_top_docs} Documents)')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Documents')\n",
    "    plt.xticks(rotation = 45, ha = 'right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Visualization\\\\tfidf_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Bar chart of top features across the corpus\n",
    "    plt.figure(figsize = (12, 6))\n",
    "    top_features_scores = [feature_importance[i] for i in top_feature_indices]\n",
    "    plt.bar(top_features, top_features_scores)\n",
    "    plt.title('Top TF-IDF Features Across All Documents')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Sum of TF-IDF Scores')\n",
    "    plt.xticks(rotation = 45, ha = 'right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Visualization\\\\tfidf_top_features.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd110ab-6e79-481b-9108-ac484b4acd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "word_tokens = []\n",
    "sentence_tokens = []\n",
    "pos_tags = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    sentence_tokens.append([sent.text.strip() for sent in doc.sents])\n",
    "    tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_space]\n",
    "    word_tokens.append([token.text.lower() for token in tokens])\n",
    "    pos_tags.append([spacy_pos_to_wordnet_pos(token.tag_) for token in tokens])\n",
    "\n",
    "lemmatized_tokens = [apply_lemmatization(t) for t in word_tokens]\n",
    "\n",
    "word_tokens, pos_tags, texts, labels, sentence_tokens = apply_augmentation_to_dataset(\n",
    "    lemmatized_tokens, pos_tags, texts, labels, sentence_tokens\n",
    ")\n",
    "\n",
    "joined_texts = [' '.join(tokens) for tokens in word_tokens]\n",
    "\n",
    "augmented_df = pd.DataFrame({\n",
    "    'label': labels,\n",
    "    'text': joined_texts,\n",
    "    'word_tokens': word_tokens\n",
    "})\n",
    "\n",
    "augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    augmented_df['text'], augmented_df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_texts)\n",
    "xtrain_tfidf = vectorizer.transform(train_texts)\n",
    "xtest_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "# --- Important: Do NOT fit the vectorizer again before transforming test data! ---\n",
    "tfidf_matrix = vectorizer.transform(joined_texts) \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nTF-IDF Matrix Shape (Train): {xtrain_tfidf.shape}\")\n",
    "print(f\"TF-IDF Matrix Shape (Test): {xtest_tfidf.shape}\")\n",
    "print(f\"TF-IDF Matrix Shape (Full): {tfidf_matrix.shape}\") \n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "visualize_tf_idf_heatmap(tfidf_matrix, feature_names)\n",
    "\n",
    "filename = \"Models\\\\Vectorizer.joblib\"\n",
    "joblib.dump(vectorizer, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05795c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"augmented_df.head():\")\n",
    "display(augmented_df.head())\n",
    "\n",
    "print(\"\\ndf.head():\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe295e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"augmented_df.tail():\")\n",
    "display(augmented_df.tail())\n",
    "\n",
    "print(\"\\ndf.tail():\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea632351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ndf info:\")\n",
    "display(df.info())\n",
    "\n",
    "print(\"\\naugmented_df info:\")\n",
    "display(augmented_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ndf dimentions:\")\n",
    "display(df.shape)\n",
    "\n",
    "print(\"\\naugmented_df dimentions:\")\n",
    "display(augmented_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8c0b2",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ccab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = augmented_df[\"text\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874de578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the column names\n",
    "tf.columns = [\"words\", \"tf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the most frequent words\n",
    "tf.sort_values(\"tf\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[tf[\"tf\"] > 3000].plot.bar(x=\"words\", y=\"tf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# check every row and join with these with a space\n",
    "text = \" \".join(i for i in augmented_df.text)\n",
    "wordcloud = WordCloud().generate(text)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67965868",
   "metadata": {},
   "source": [
    "# Modelling - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a699ea3",
   "metadata": {},
   "source": [
    "####        •    ML -> Logistic Regression, Naive Bayes, SVM, Decision tree, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f41270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, train_labels, feature_vector_test, test_labels, is_neural_net=False):\n",
    "    classifier.fit(feature_vector_train, train_labels)\n",
    "\n",
    "    train_predictions = classifier.predict(feature_vector_train)\n",
    "    test_predictions = classifier.predict(feature_vector_test)\n",
    "    train_accuracy = metrics.accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = metrics.accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return test_accuracy  # Return the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9d515",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1 = linear_model.LogisticRegression(penalty='l1', C=0.7, solver='liblinear', random_state=42)\n",
    "model_l1.fit(xtrain_tfidf, train_labels)\n",
    "\n",
    "accuracy = train_model(model_l1, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename_l1 = r\"Models\\LR_L1.joblib\"\n",
    "joblib.dump(model_l1, filename_l1)\n",
    "\n",
    "with open(r\"Accuracies\\LR_L1.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2 = linear_model.LogisticRegression(penalty='l2', C=0.7, solver='liblinear', random_state=42)\n",
    "model_l2.fit(xtrain_tfidf, train_labels)\n",
    "\n",
    "accuracy = train_model(model_l2, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename_l2 = r\"Models\\LR_L2.joblib\"\n",
    "joblib.dump(model_l2, filename_l2)\n",
    "\n",
    "with open(r\"Accuracies\\LR_L2.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00af1b",
   "metadata": {},
   "source": [
    "#### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6801ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = naive_bayes.MultinomialNB(alpha=0.7)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\NB.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\NB.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1b670",
   "metadata": {},
   "source": [
    "#### 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear', C=0.3)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\SVM_Linear.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\SVM_Linear.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', C=0.3)\n",
    "model.fit(xtrain_tfidf, train_labels)\n",
    "\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "joblib.dump(model, r\"Models\\SVM_rbf.joblib\")\n",
    "\n",
    "with open(r\"Accuracies\\SVM_rbf.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3eefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='poly', C=0.3, degree=4)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\SVM_Poly.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\SVM_Poly.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e209c",
   "metadata": {},
   "source": [
    "#### 4. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=3, random_state=42)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\DT.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\DT.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba9625",
   "metadata": {},
   "source": [
    "#### 5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\RF.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\RF.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d47c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_confusion_matrix(name, model, X_test, y_test):\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', cbar=False)\n",
    "        plt.title(f'Confusion Matrix for {name}')\n",
    "        plt.xlabel('Predicted Label (0=Negative, 1=Positive)')\n",
    "        plt.ylabel('True Label (0=Negative, 1=Positive)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        sanitized_name = name.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "        plt.savefig(f'Visualization/confusion_matrix_{sanitized_name}.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating confusion matrix for {name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression (L1)': joblib.load('Models/LR_L1.joblib'),\n",
    "    'Logistic Regression (L2)': joblib.load('Models/LR_L2.joblib'),\n",
    "    'Naive Bayes': joblib.load('Models/NB.joblib'),\n",
    "    'SVM (Linear)': joblib.load('Models/SVM_Linear.joblib'),\n",
    "    'SVM (RBF)': joblib.load('Models/SVM_rbf.joblib'),\n",
    "    'SVM (Poly)': joblib.load('Models/SVM_Poly.joblib'),\n",
    "    'Decision Tree': joblib.load('Models/DT.joblib'),\n",
    "    'Random Forest': joblib.load('Models/RF.joblib')\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print_confusion_matrix(name, model, xtest_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe38d91",
   "metadata": {},
   "source": [
    "#### • DL -> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same train/test split for BERT\n",
    "bert_train_texts = train_texts.tolist()\n",
    "bert_test_texts = test_texts.tolist()\n",
    "bert_train_labels = train_labels.tolist()\n",
    "bert_test_labels = test_labels.tolist()\n",
    "\n",
    "bert_train_ds = Dataset.from_dict({\"text\": bert_train_texts, \"label\": bert_train_labels})\n",
    "bert_test_ds = Dataset.from_dict({\"text\": bert_test_texts, \"label\": bert_test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "bert_train_ds = bert_train_ds.map(tokenize_function, batched=True)\n",
    "bert_test_ds = bert_test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set PyTorch format\n",
    "bert_train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "bert_test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addefa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT for classification\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define evaluation metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aadf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_train_ds,\n",
    "    eval_dataset=bert_test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ffc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BERT model on test set\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"BERT Classification Accuracy: {metrics['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cf194",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"Visualization\", exist_ok=True)\n",
    "\n",
    "vectorizer = joblib.load(\"Models/Vectorizer.joblib\")\n",
    "\n",
    "xtrain_tfidf = vectorizer.transform(train_texts)\n",
    "xtest_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "print(f\"Recomputed TF-IDF Matrix Shape (Train): {xtrain_tfidf.shape}\")\n",
    "print(f\"Recomputed TF-IDF Matrix Shape (Test): {xtest_tfidf.shape}\")\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression (L1)': joblib.load('Models/LR_L1.joblib'),\n",
    "    'Logistic Regression (L2)': joblib.load('Models/LR_L2.joblib'),\n",
    "    'Naive Bayes': joblib.load('Models/NB.joblib'),\n",
    "    'SVM (Linear)': joblib.load('Models/SVM_Linear.joblib'),\n",
    "    'SVM (RBF)': joblib.load('Models/SVM_rbf.joblib'),\n",
    "    'SVM (Poly)': joblib.load('Models/SVM_Poly.joblib'),\n",
    "    'Decision Tree': joblib.load('Models/DT.joblib'),\n",
    "    'Random Forest': joblib.load('Models/RF.joblib')\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, sk_model in models.items():\n",
    "    train_pred = sk_model.predict(xtrain_tfidf)\n",
    "    test_pred = sk_model.predict(xtest_tfidf)\n",
    "    train_accuracy = accuracy_score(train_labels, train_pred)\n",
    "    test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Test Accuracy': test_accuracy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='Train Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Model Accuracy Comparison (Sorted by Train Accuracy)\")\n",
    "display(results_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(results_df))\n",
    "\n",
    "plt.bar(index, results_df['Train Accuracy'], bar_width, label='Train Accuracy', color='skyblue')\n",
    "plt.bar([i + bar_width for i in index], results_df['Test Accuracy'], bar_width, label='Test Accuracy', color='lightcoral')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy Comparison Across Models')\n",
    "plt.xticks([i + bar_width / 2 for i in index], results_df['Model'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Visualization/model_accuracy_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['blue', 'red', 'green', 'purple', 'orange', 'brown', 'pink', 'gray']\n",
    "\n",
    "for (model_name, sk_model), color in zip(models.items(), colors):\n",
    "    if hasattr(sk_model, 'predict_proba'):\n",
    "        probs = sk_model.predict_proba(xtest_tfidf)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(test_labels, probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})', color=color)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Scikit-learn Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Visualization/roc_curve_all_models.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model_files = {\n",
    "    'LR_L1': 'Models/LR_L1.joblib',\n",
    "    'LR_L2': 'Models/LR_L2.joblib',\n",
    "    'Naive Bayes': 'Models/NB.joblib',\n",
    "    'SVM_Linear': 'Models/SVM_Linear.joblib',\n",
    "    'SVM_RBF': 'Models/SVM_rbf.joblib',\n",
    "    'SVM_Poly': 'Models/SVM_Poly.joblib',\n",
    "    'Decision Tree': 'Models/DT.joblib',\n",
    "    'Random Forest': 'Models/RF.joblib'\n",
    "}\n",
    "\n",
    "tsne_tfidf = TSNE(n_components=2, random_state=42, perplexity=30, n_jobs=-1)\n",
    "tsne_tfidf_results = tsne_tfidf.fit_transform(xtest_tfidf.toarray())\n",
    "\n",
    "tsne_tfidf_df = pd.DataFrame({\n",
    "    'TSNE1': tsne_tfidf_results[:, 0],\n",
    "    'TSNE2': tsne_tfidf_results[:, 1],\n",
    "    'True Label': test_labels\n",
    "})\n",
    "\n",
    "test_texts_reset = test_texts.reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='True Label', palette='coolwarm', data=tsne_tfidf_df, alpha=0.7)\n",
    "plt.title('t-SNE Visualization of TF-IDF Features (True Labels)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='True Label (0=Negative, 1=Positive)', loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in np.random.choice(len(test_texts_reset), 5, replace=False):\n",
    "    plt.text(tsne_tfidf_df['TSNE1'].iloc[i], tsne_tfidf_df['TSNE2'].iloc[i], \n",
    "             test_texts_reset.iloc[i][:20], fontsize=8, ha='center')\n",
    "\n",
    "plt.savefig('Visualization/tsne_tfidf_true.png')\n",
    "plt.show()\n",
    "\n",
    "for model_name, model_path in model_files.items():\n",
    "    if os.path.exists(model_path):\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        predictions = model.predict(xtest_tfidf)\n",
    "        tsne_tfidf_df['Predicted Label'] = predictions\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='TSNE1', y='TSNE2', hue='Predicted Label', palette='coolwarm', data=tsne_tfidf_df, alpha=0.7)\n",
    "        plt.title(f't-SNE Visualization of TF-IDF Features ({model_name} Predictions)')\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        plt.legend(title='Predicted Label (0=Negative, 1=Positive)', loc='best')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        np.random.seed(42)\n",
    "        for i in np.random.choice(len(test_texts_reset), 5, replace=False):\n",
    "            plt.text(tsne_tfidf_df['TSNE1'].iloc[i], tsne_tfidf_df['TSNE2'].iloc[i], \n",
    "                     test_texts_reset.iloc[i][:20], fontsize=8, ha='center')\n",
    "\n",
    "        plt.savefig(f'Visualization/tsne_tfidf_{model_name.replace(\" \", \"_\")}_predictions.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Model file {model_path} not found. Skipping {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_tfidf = PCA(n_components=2, random_state=42)\n",
    "pca_tfidf_results = pca_tfidf.fit_transform(xtest_tfidf.toarray())\n",
    "\n",
    "pca_tfidf_df = pd.DataFrame({\n",
    "    'PC1': pca_tfidf_results[:, 0],\n",
    "    'PC2': pca_tfidf_results[:, 1],\n",
    "    'True Label': test_labels\n",
    "})\n",
    "\n",
    "test_texts_reset = test_texts.reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='True Label', palette='coolwarm', data=pca_tfidf_df, alpha=0.7)\n",
    "plt.title('PCA Visualization of TF-IDF Features (True Labels)')\n",
    "plt.xlabel(f'PC1 ({pca_tfidf.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca_tfidf.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.legend(title='True Label (0=Negative, 1=Positive)', loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in np.random.choice(len(test_texts_reset), 5, replace=False):\n",
    "    plt.text(pca_tfidf_df['PC1'].iloc[i], pca_tfidf_df['PC2'].iloc[i], \n",
    "             test_texts_reset.iloc[i][:20], fontsize=8, ha='center')\n",
    "\n",
    "plt.savefig('Visualization/pca_tfidf_true.png')\n",
    "plt.show()\n",
    "\n",
    "for model_name, model_path in model_files.items():\n",
    "    if os.path.exists(model_path):\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        predictions = model.predict(xtest_tfidf)\n",
    "        pca_tfidf_df['Predicted Label'] = predictions\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='PC1', y='PC2', hue='Predicted Label', palette='coolwarm', data=pca_tfidf_df, alpha=0.7)\n",
    "        plt.title(f'PCA Visualization of TF-IDF Features ({model_name} Predictions)')\n",
    "        plt.xlabel(f'PC1 ({pca_tfidf.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca_tfidf.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.legend(title='Predicted Label (0=Negative, 1=Positive)', loc='best')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        np.random.seed(42)\n",
    "        for i in np.random.choice(len(test_texts_reset), 5, replace=False):\n",
    "            plt.text(pca_tfidf_df['PC1'].iloc[i], pca_tfidf_df['PC2'].iloc[i], \n",
    "                     test_texts_reset.iloc[i][:20], fontsize=8, ha='center')\n",
    "\n",
    "        plt.savefig(f'Visualization/pca_tfidf_{model_name.replace(\" \", \"_\")}_predictions.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Model file {model_path} not found. Skipping {model_name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
