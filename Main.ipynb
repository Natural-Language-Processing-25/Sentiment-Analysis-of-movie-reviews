{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "09cc8815-b160-406c-a3ae-17e9d8765864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NLP Project    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c6d84703-7454-4dd4-b1c8-9a4d5b6d5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_to_wordnet_pos(spacy_pos):\n",
    "    if spacy_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif spacy_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif spacy_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif spacy_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b8f35a43-5c47-49ad-8cc6-95ffa03211cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    synonyms.discard(word)      # Remove the original word to avoid replacement with itself\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "161f45fe-6246-4ff3-895c-85d4092ca01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_text_with_synonyms(tokens, pos_tags, synonym_probability=0.2):\n",
    "    augmented_tokens = []\n",
    "    \n",
    "    for token, pos_tag in zip(tokens, pos_tags):\n",
    "        if random.random() < synonym_probability:\n",
    "            if pos_tag in ['n', 'v', 'a']:   #nouns adjectives and verbs \n",
    "                synonyms = get_synonyms(token)\n",
    "                if synonyms:\n",
    "                    new_word = random.choice(synonyms)\n",
    "                    augmented_tokens.append(new_word)\n",
    "                    continue  \n",
    "        augmented_tokens.append(token)         # Add the original word if no augmentation is done\n",
    "    \n",
    "    return augmented_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "33efe070-6a00-4518-b544-7ee483699d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation_to_dataset(word_tokens, pos_tags, texts, labels, sentence_tokens, synonym_probability=0.2):\n",
    "    all_word_tokens = word_tokens.copy()\n",
    "    all_texts = texts.copy()  # Original text is preserved\n",
    "    all_labels = labels.copy()\n",
    "    all_sentence_tokens = sentence_tokens.copy()\n",
    "    all_pos_tags = pos_tags.copy()\n",
    "\n",
    "    for tokens, pos, label, sentence, text in zip(word_tokens, pos_tags, labels, sentence_tokens, texts):\n",
    "        augmented_tokens = augment_text_with_synonyms(tokens, pos, synonym_probability)\n",
    "\n",
    "        all_word_tokens.append(augmented_tokens)\n",
    "        all_texts.append(text)  # Keep the original text\n",
    "        all_labels.append(label)\n",
    "        all_pos_tags.append(pos)\n",
    "        all_sentence_tokens.append(sentence)  # Sentence tokens are not augmented\n",
    "\n",
    "    return all_word_tokens, all_pos_tags, all_texts, all_labels, all_sentence_tokens\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f4aac42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(tokens):\n",
    "    return [token.lemma_ for token in nlp(' '.join(tokens))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c3d4bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(text):\n",
    "    \n",
    "    words = text.split()\n",
    "    return ' '.join([PorterStemmer().stem(word) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "695072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tfidf_heatmap(tfidf_matrix, feature_names, n_top_features=20, n_top_docs=10):\n",
    "    # Get the top features by summing TF-IDF scores across documents\n",
    "    tfidf_array = tfidf_matrix.toarray()\n",
    "    feature_importance = np.sum(tfidf_array, axis=0)\n",
    "    top_feature_indices = np.argsort(feature_importance)[-n_top_features:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_feature_indices]\n",
    "    \n",
    "    # Get the top documents by summing TF-IDF scores across features\n",
    "    doc_importance = np.sum(tfidf_array, axis=1)\n",
    "    top_doc_indices = np.argsort(doc_importance)[-n_top_docs:][::-1]\n",
    "    \n",
    "    # Extract the submatrix for visualization\n",
    "    sub_matrix = tfidf_array[np.ix_(top_doc_indices, top_feature_indices)]\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        sub_matrix,\n",
    "        annot=True,          # Show values in cells\n",
    "        fmt='.3f',           # Format with 3 decimal places\n",
    "        cmap='YlGnBu',       # Better colormap\n",
    "        xticklabels=top_features,\n",
    "        yticklabels=range(n_top_docs)\n",
    "    )\n",
    "    plt.title(f'TF-IDF Heatmap (Top {n_top_features} Features, First {n_top_docs} Documents)')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Documents')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tfidf_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Additional visualization: Show top words for each document\n",
    "    print(\"\\nTop words for each document:\")\n",
    "    for i, doc_idx in enumerate(top_doc_indices[:5]):  # Show for first 5 documents\n",
    "        doc_scores = tfidf_array[doc_idx]\n",
    "        top_word_indices = np.argsort(doc_scores)[-10:][::-1]  # Top 10 words\n",
    "        top_words = [(feature_names[idx], doc_scores[idx]) for idx in top_word_indices if doc_scores[idx] > 0]\n",
    "        \n",
    "        print(f\"Document {i}: {', '.join([f'{word} ({score:.3f})' for word, score in top_words])}\")\n",
    "    \n",
    "    # Also create a bar chart of top features across the corpus\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_features_scores = [feature_importance[i] for i in top_feature_indices]\n",
    "    plt.bar(top_features, top_features_scores)\n",
    "    plt.title('Top TF-IDF Features Across All Documents')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Sum of TF-IDF Scores')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tfidf_top_features.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7bd110ab-6e79-481b-9108-ac484b4acd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_dataset():\n",
    "    texts = []\n",
    "    labels = []\n",
    "    word_tokens = []\n",
    "    sentence_tokens = []\n",
    "    pos_tags = []\n",
    "    \n",
    "    base_path = Path('review_polarity/txt_sentoken')\n",
    "    pos_path = base_path / 'pos'\n",
    "    neg_path = base_path / 'neg'\n",
    "    \n",
    "    if pos_path.exists():\n",
    "        for file in pos_path.glob('*.txt'):\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                texts.append(text)\n",
    "\n",
    "                doc = nlp(text)\n",
    "                sentence_tokens.append([sent.text.strip() for sent in doc.sents])\n",
    "                tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_space]\n",
    "                word_tokens.append([token.text.lower() for token in tokens])  \n",
    "                pos_tags.append([spacy_pos_to_wordnet_pos(token.tag_) for token in tokens])\n",
    "                    \n",
    "\n",
    "\n",
    "                labels.append(1)\n",
    "\n",
    "    \n",
    "    if neg_path.exists():\n",
    "        for file in neg_path.glob('*.txt'):\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                texts.append(text)\n",
    "\n",
    "                doc = nlp(text)\n",
    "                sentence_tokens.append([sent.text.strip() for sent in doc.sents])\n",
    "                tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_space]\n",
    "                word_tokens.append([token.text.lower() for token in tokens])  \n",
    "                pos_tags.append([spacy_pos_to_wordnet_pos(token.tag_) for token in tokens])\n",
    "\n",
    "                labels.append(0)\n",
    "                \n",
    "    lemmatized_tokens = [apply_lemmatization(t) for t in word_tokens]\n",
    "    print(\"Sample lemmatized tokens:\")\n",
    "    for i in range(3):\n",
    "        print(lemmatized_tokens[i][:20])\n",
    "\n",
    "    word_tokens, pos_tags, texts, labels, sentence_tokens = apply_augmentation_to_dataset(lemmatized_tokens, pos_tags, texts, labels, sentence_tokens)\n",
    "    \n",
    "    joined_texts = [' '.join(tokens) for tokens in word_tokens]\n",
    "    print(\"Sample joined tokens:\")\n",
    "    for i in range(3):\n",
    "        print(joined_texts[i][:100])\n",
    "        \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(joined_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nTF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    visualize_tfidf_heatmap(tfidf_matrix, feature_names)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'word_tokens': word_tokens,\n",
    "        'sentence_tokens': sentence_tokens,\n",
    "        'label': labels,\n",
    "        'pos_tags': pos_tags\n",
    "    })\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(df.head)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2271670b-720b-427d-b499-8e6ac5fb7bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample lemmatized tokens:\n",
      "['film', 'adapt', 'comic', 'book', 'plenty', 'success', 'superhero', 'batman', 'superman', 'spawn', 'gear', 'kid', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'comic', 'book', 'like']\n",
      "['movie', 'come', 'suspect', 'studio', 'indication', 'stinker', 'everybody', 'surprise', 'studio', 'film', 'critical', 'darling', 'mtv', 'film', 'election', 'high', 'school', 'comedy', 'star', 'matthew']\n",
      "['get', 'mail', 'work', 'alot', 'well', 'deserve', 'order', 'film', 'success', 'cast', 'extremely', 'popular', 'attractive', 'star', 'share', 'screen', 'hour', 'collect', 'profit', 'real']\n",
      "Sample joined tokens:\n",
      "film adapt comic book plenty success superhero batman superman spawn gear kid casper arthouse crowd \n",
      "movie come suspect studio indication stinker everybody surprise studio film critical darling mtv fil\n",
      "get mail work alot well deserve order film success cast extremely popular attractive star share scre\n",
      "\n",
      "TF-IDF Matrix Shape: (4000, 42955)\n",
      "Number of features: 42955\n",
      "\n",
      "Top words for each document:\n",
      "Document 0: boogie (0.458), dirk (0.270), mr (0.206), night (0.192), anderson (0.141), film (0.123), jack (0.122), horner (0.110), nighttime (0.105), reed (0.089)\n",
      "Document 1: boogie (0.448), dirk (0.265), night (0.247), mr (0.214), anderson (0.206), jack (0.146), film (0.136), reed (0.109), horner (0.108), industry (0.094)\n",
      "Document 2: forward (0.275), osmet (0.165), pay (0.151), hunt (0.141), leder (0.133), mohr (0.118), academy (0.113), glame (0.110), helen (0.102), house_trailer (0.093)\n",
      "Document 3: state_of_war (0.165), star (0.160), george_lucas (0.159), lucas (0.157), sf (0.149), science (0.138), george (0.134), fiction (0.134), movie (0.131), hollywood (0.127)\n",
      "Document 4: scream (0.307), williamson (0.268), film (0.204), sidney (0.179), gale (0.179), dewey (0.142), mr (0.140), slasher (0.108), continuation (0.099), killer (0.099)\n",
      "<bound method NDFrame.head of                                                    text  \\\n",
      "0     harmless , silly and fun comedy about dim-witt...   \n",
      "1     at one point in this movie there is a staging ...   \n",
      "2     sick : the life and death of bob flanagan , su...   \n",
      "3     everybody in this film's thinking of alicia . ...   \n",
      "4     lisa cholodenko's \" high art , \" is an intelli...   \n",
      "...                                                 ...   \n",
      "3995  i never understood what the clich ? \" hell on ...   \n",
      "3996  in this re-make of the 1954 japanese monster f...   \n",
      "3997  the verdict : spine-chilling drama from horror...   \n",
      "3998  midway through \" anaconda \" , documentary film...   \n",
      "3999  starship troopers is a bad movie . \\ni mean , ...   \n",
      "\n",
      "                                            word_tokens  \\\n",
      "0     [harmless, silly, fun, comedy, dim, witted, wr...   \n",
      "1     [point, movie, staging, opera, go, completely,...   \n",
      "2     [sick, life, death, bob, flanagan, supermasoch...   \n",
      "3     [everybody, photographic_film, think, alicia, ...   \n",
      "4     [lisa, cholodenko, high, art, intelligent, tra...   \n",
      "...                                                 ...   \n",
      "3995  [understand, clich, hell, earth, truly, mean, ...   \n",
      "3996  [1954, japanese, monster, film, godzilla, tran...   \n",
      "3997  [verdict, spine, chill, drama, horror, maestro...   \n",
      "3998  [midway, anaconda, documentary_film, filmmaker...   \n",
      "3999  [starship, trooper, bad, movie, mean, bad, mov...   \n",
      "\n",
      "                                        sentence_tokens  label  \\\n",
      "0     [harmless , silly and fun comedy about dim-wit...      1   \n",
      "1     [at one point in this movie there is a staging...      0   \n",
      "2     [sick : the life and death of bob flanagan , s...      1   \n",
      "3     [everybody in this film's thinking of alicia ....      0   \n",
      "4     [lisa cholodenko's \" high art , \" is an intell...      1   \n",
      "...                                                 ...    ...   \n",
      "3995  [i never understood what the clich ? \", hell o...      0   \n",
      "3996  [in this re-make of the 1954 japanese monster ...      0   \n",
      "3997  [the verdict : spine-chilling drama from horro...      1   \n",
      "3998  [midway through \" anaconda \" , documentary fil...      0   \n",
      "3999  [starship troopers is a bad movie ., i mean , ...      0   \n",
      "\n",
      "                                               pos_tags  \n",
      "0     [a, a, a, n, n, a, n, n, n, n, n, n, n, n, v, ...  \n",
      "1     [n, n, n, n, v, r, a, n, n, v, n, v, n, v, v, ...  \n",
      "2     [a, n, n, n, n, n, n, n, v, n, n, n, v, n, n, ...  \n",
      "3     [n, n, n, n, n, v, n, n, a, n, n, v, n, n, n, ...  \n",
      "4     [n, n, a, n, a, a, n, a, n, r, n, a, n, a, n, ...  \n",
      "...                                                 ...  \n",
      "3995  [v, n, n, n, r, v, r, v, n, a, a, r, a, v, v, ...  \n",
      "3996  [n, a, n, n, n, v, a, n, n, v, n, n, n, n, a, ...  \n",
      "3997  [n, n, v, n, n, n, n, n, v, a, n, v, n, a, n, ...  \n",
      "3998  [r, n, a, n, n, n, n, r, v, n, n, v, v, n, a, ...  \n",
      "3999  [n, n, a, n, v, a, n, v, n, a, n, v, n, n, n, ...  \n",
      "\n",
      "[4000 rows x 5 columns]>\n",
      "\n",
      "Dataset shape: (4000, 5)\n",
      "\n",
      "Label distribution:\n",
      "1    2000\n",
      "0    2000\n",
      "Name: label, dtype: int64\n",
      "Tokens in the first 20 rows:\n",
      "0     [harmless, silly, fun, comedy, dim, witted, wr...\n",
      "1     [point, movie, staging, opera, go, completely,...\n",
      "2     [sick, life, death, bob, flanagan, supermasoch...\n",
      "3     [everybody, photographic_film, think, alicia, ...\n",
      "4     [lisa, cholodenko, high, art, intelligent, tra...\n",
      "5     [close, eye, moment, imagine, sound, stephen, ...\n",
      "6     [jet, li, bust, american, action, movie, scene...\n",
      "7     [success, surprise, hit, alien, direct, ridley...\n",
      "8     [jay, silent, bob, strike, kevin, smith, swan,...\n",
      "9     [contact, pg, moment, late, robert, zemeckis, ...\n",
      "10    [capsulate, super, light, situation, comedy, s...\n",
      "11    [brian, Delaware, palma, snake, eye, star, nic...\n",
      "12    [house, bear, bad, go, haunt, tag, line, add, ...\n",
      "13    [think, responsible, citizen, stop, jim, carre...\n",
      "14    [haunt, film, confuse, forget, true, intend, h...\n",
      "15    [utmost, respect, richard, dreyfuss, actor, pr...\n",
      "16    [animal, marginally, inspire, comedy, manage, ...\n",
      "17    [costume, drama, set, 1500s, england, elizabet...\n",
      "18    [feel, hate, letter, pour, folk, love, wedding...\n",
      "19    [sort, warp, critical, nightmare, good, movie,...\n",
      "Name: word_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the dataset\n",
    "    df = create_sentiment_dataset()\n",
    "    \n",
    "    # Display first few rows and basic information\n",
    "    # print(\"\\nFirst few rows of the dataset:\")\n",
    "    # df.head()\n",
    "\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    # print(df.head(20))\n",
    "    # Print the first 20 rows of the dataframe\n",
    "    # Print tokens from the first 20 rows\n",
    "    print(\"Tokens in the first 20 rows:\")\n",
    "    print(df['word_tokens'].head(20))\n",
    "    \n",
    "    # Print tokens from rows with index 2000 to 2020\n",
    "    # print(\"\\nTokens from index 2000 to 2020:\")\n",
    "    # print(df['word_tokens'].iloc[2000:2021])\n",
    "\n",
    "\n",
    "    # # # Display tokenization examples\n",
    "    # print(\"\\nExample of word tokenization for first review:\")\n",
    "    # print(\"Number of words:\", len(df['word_tokens'][0]))\n",
    "    # print(\"First 20 words:\", df['word_tokens'][0][:20])\n",
    "    \n",
    "    # print(\"\\nExample of sentence tokenization for first review:\")\n",
    "    # print(\"Number of sentences:\", len(df['sentence_tokens'][0]))\n",
    "    # print(\"First 2 sentences:\", df['sentence_tokens'][0][:2])  \n",
    "\n",
    "    # print(\"\\nExample of POS tags for first review:\")\n",
    "    # print(\"POS tags:\", df['pos_tags'][0][:20])  # Display first 20 POS tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
