{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1595eb27",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e1aabd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.3 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 1)) (1.24.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas==1.5.3 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: spacy==3.7.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 3)) (3.7.2)\n",
      "Requirement already satisfied: nltk==3.8.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 4)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: matplotlib==3.8.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 6)) (3.8.2)\n",
      "Requirement already satisfied: seaborn==0.13.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 7)) (0.13.0)\n",
      "Requirement already satisfied: ipython==8.18.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 8)) (8.18.1)\n",
      "Requirement already satisfied: datasets==2.16.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 9)) (2.16.1)\n",
      "Requirement already satisfied: transformers==4.36.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 10)) (4.36.2)\n",
      "Requirement already satisfied: accelerate==0.25.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 11)) (0.25.0)\n",
      "Requirement already satisfied: torch==2.1.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 13)) (2.1.2)\n",
      "Requirement already satisfied: thinc<8.3.6 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 14)) (8.2.5)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from -r requirements.txt (line 12)) (0.31.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (80.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from spacy==3.7.2->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: click in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (8.2.0)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (2024.11.6)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 5)) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from matplotlib==3.8.2->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (5.9.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (3.15.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (0.3.7)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1->-r requirements.txt (line 9)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (3.11.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from datasets==2.16.1->-r requirements.txt (line 9)) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from transformers==4.36.2->-r requirements.txt (line 10)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from transformers==4.36.2->-r requirements.txt (line 10)) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from accelerate==0.25.0->-r requirements.txt (line 11)) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from torch==2.1.2->-r requirements.txt (line 13)) (4.13.2)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from torch==2.1.2->-r requirements.txt (line 13)) (1.12.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from torch==2.1.2->-r requirements.txt (line 13)) (3.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from huggingface_hub[hf_xet]->-r requirements.txt (line 12)) (1.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from thinc<8.3.6->-r requirements.txt (line 14)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from thinc<8.3.6->-r requirements.txt (line 14)) (0.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 9)) (1.20.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython==8.18.1->-r requirements.txt (line 8)) (0.8.3)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython==8.18.1->-r requirements.txt (line 8)) (0.2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 3)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2->-r requirements.txt (line 3)) (2025.4.26)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from jinja2->spacy==3.7.2->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython==8.18.1->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython==8.18.1->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\rahma\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython==8.18.1->-r requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from sympy->torch==2.1.2->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\anaconda\\envs\\deployment\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2->-r requirements.txt (line 3)) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# # You may need to run those in your enviroment terminal.\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b6641483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rahma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rahma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a5438",
   "metadata": {},
   "source": [
    "Creating data frame of the data and assigning them labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "06d524bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>if anything , \" stigmata \" should be taken as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>john boorman's \" zardoz \" is a goofy cinematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>the kids in the hall are an acquired taste . \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>two party guys bob their heads to haddaway's d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1995      0  if anything , \" stigmata \" should be taken as ...\n",
       "1996      0  john boorman's \" zardoz \" is a goofy cinematic...\n",
       "1997      0  the kids in the hall are an acquired taste . \\...\n",
       "1998      0  there was a time when john carpenter was a gre...\n",
       "1999      0  two party guys bob their heads to haddaway's d..."
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "base_path = Path('review_polarity/txt_sentoken')\n",
    "pos_path = base_path / 'pos'\n",
    "neg_path = base_path / 'neg'\n",
    "\n",
    "# Assign label 1\n",
    "if pos_path.exists():\n",
    "    for file in pos_path.glob('*.txt'):\n",
    "        with open(file, 'r', encoding = 'utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)\n",
    "\n",
    "# Assign label 0\n",
    "if neg_path.exists():\n",
    "    for file in neg_path.glob('*.txt'):\n",
    "        with open(file, 'r', encoding = 'utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'label': labels,\n",
    "    'text': texts\n",
    "})\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ad59ad76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1716a",
   "metadata": {},
   "source": [
    "No duplicates were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c6d84703-7454-4dd4-b1c8-9a4d5b6d5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_to_wordnet_pos(spacy_pos):\n",
    "    if spacy_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif spacy_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif spacy_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif spacy_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b8f35a43-5c47-49ad-8cc6-95ffa03211cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    synonyms.discard(word)      # Remove the original word to avoid replacement with itself\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "161f45fe-6246-4ff3-895c-85d4092ca01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_text_with_synonyms(tokens, pos_tags, synonym_probability = 0.2):\n",
    "    augmented_tokens = []\n",
    "\n",
    "    for token, pos_tag in zip(tokens, pos_tags):\n",
    "        if random.random() < synonym_probability:\n",
    "            if pos_tag in ['n', 'v', 'a']:   #nouns adjectives and verbs\n",
    "                synonyms = get_synonyms(token)\n",
    "                if synonyms:\n",
    "                    new_word = random.choice(synonyms)\n",
    "                    augmented_tokens.append(new_word)\n",
    "                    continue\n",
    "        augmented_tokens.append(token)         # Add the original word if no augmentation is done\n",
    "\n",
    "    return augmented_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "33efe070-6a00-4518-b544-7ee483699d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation_to_dataset(word_tokens, pos_tags, texts, labels, sentence_tokens, synonym_probability = 0.2):\n",
    "    all_word_tokens = word_tokens.copy()\n",
    "    all_texts = texts.copy()  # Original text is preserved\n",
    "    all_labels = labels.copy()\n",
    "    all_sentence_tokens = sentence_tokens.copy()\n",
    "    all_pos_tags = pos_tags.copy()\n",
    "\n",
    "    for tokens, pos, label, sentence, text in zip(word_tokens, pos_tags, labels, sentence_tokens, texts):\n",
    "        augmented_tokens = augment_text_with_synonyms(tokens, pos, synonym_probability)\n",
    "\n",
    "        all_word_tokens.append(augmented_tokens)\n",
    "        all_texts.append(text)  # Keep the original text\n",
    "        all_labels.append(label)\n",
    "        all_pos_tags.append(pos)\n",
    "        all_sentence_tokens.append(sentence)  # Sentence tokens are not augmented\n",
    "\n",
    "    return all_word_tokens, all_pos_tags, all_texts, all_labels, all_sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f4aac42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(tokens):\n",
    "    return [token.lemma_ for token in nlp(' '.join(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c3d4bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(text):\n",
    "\n",
    "    words = text.split()\n",
    "    return ' '.join([PorterStemmer().stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "695072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tf_idf_heatmap(tfidf_matrix, feature_names, n_top_features = 20, n_top_docs = 10):\n",
    "    # Get the top features by summing TF-IDF scores across documents\n",
    "    tfidf_array = tfidf_matrix.toarray()\n",
    "    feature_importance = np.sum(tfidf_array, axis = 0)\n",
    "    top_feature_indices = np.argsort(feature_importance)[-n_top_features:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_feature_indices]\n",
    "\n",
    "    # Get the top documents by summing TF-IDF scores across features\n",
    "    doc_importance = np.sum(tfidf_array, axis = 1)\n",
    "    top_doc_indices = np.argsort(doc_importance)[-n_top_docs:][::-1]\n",
    "\n",
    "    # Extract the submatrix for visualization\n",
    "    sub_matrix = tfidf_array[np.ix_(top_doc_indices, top_feature_indices)]\n",
    "\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    sns.heatmap(\n",
    "        sub_matrix,\n",
    "        annot = True,          # Show values in cells\n",
    "        fmt = '.3f',           # Format with 3 decimal places\n",
    "        cmap = 'YlGnBu',       # Better colormap\n",
    "        xticklabels = top_features,\n",
    "        yticklabels = range(n_top_docs)\n",
    "    )\n",
    "    plt.title(f'TF-IDF Heatmap (Top {n_top_features} Features, First {n_top_docs} Documents)')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Documents')\n",
    "    plt.xticks(rotation = 45, ha = 'right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Visualization\\\\tfidf_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Bar chart of top features across the corpus\n",
    "    plt.figure(figsize = (12, 6))\n",
    "    top_features_scores = [feature_importance[i] for i in top_feature_indices]\n",
    "    plt.bar(top_features, top_features_scores)\n",
    "    plt.title('Top TF-IDF Features Across All Documents')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Sum of TF-IDF Scores')\n",
    "    plt.xticks(rotation = 45, ha = 'right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Visualization\\\\tfidf_top_features.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7bd110ab-6e79-481b-9108-ac484b4acd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix Shape (Train): (3200, 41197)\n",
      "TF-IDF Matrix Shape (Test): (800, 41197)\n",
      "TF-IDF Matrix Shape (Full): (4000, 41197)\n",
      "Number of features: 41197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Models\\\\Vectorizer.joblib']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "word_tokens = []\n",
    "sentence_tokens = []\n",
    "pos_tags = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    sentence_tokens.append([sent.text.strip() for sent in doc.sents])\n",
    "    tokens = [token for token in doc if not token.is_punct and not token.is_stop and not token.is_space]\n",
    "    word_tokens.append([token.text.lower() for token in tokens])\n",
    "    pos_tags.append([spacy_pos_to_wordnet_pos(token.tag_) for token in tokens])\n",
    "\n",
    "lemmatized_tokens = [apply_lemmatization(t) for t in word_tokens]\n",
    "\n",
    "word_tokens, pos_tags, texts, labels, sentence_tokens = apply_augmentation_to_dataset(\n",
    "    lemmatized_tokens, pos_tags, texts, labels, sentence_tokens\n",
    ")\n",
    "\n",
    "joined_texts = [' '.join(tokens) for tokens in word_tokens]\n",
    "\n",
    "augmented_df = pd.DataFrame({\n",
    "    'label': labels,\n",
    "    'text': joined_texts,\n",
    "    'word_tokens': word_tokens\n",
    "})\n",
    "\n",
    "augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    augmented_df['text'], augmented_df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_texts)\n",
    "xtrain_tfidf = vectorizer.transform(train_texts)\n",
    "xtest_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "# --- Important: Do NOT fit the vectorizer again before transforming test data! ---\n",
    "tfidf_matrix = vectorizer.transform(joined_texts) \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nTF-IDF Matrix Shape (Train): {xtrain_tfidf.shape}\")\n",
    "print(f\"TF-IDF Matrix Shape (Test): {xtest_tfidf.shape}\")\n",
    "print(f\"TF-IDF Matrix Shape (Full): {tfidf_matrix.shape}\") \n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "visualize_tf_idf_heatmap(tfidf_matrix, feature_names)\n",
    "\n",
    "filename = \"Models\\\\Vectorizer.joblib\"\n",
    "joblib.dump(vectorizer, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "05795c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented_df.head():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>harmless silly fun comedy dim witte wrestle fa...</td>\n",
       "      <td>[harmless, silly, fun, comedy, dim, witte, wre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>spot motion-picture_show staging opera go comp...</td>\n",
       "      <td>[spot, motion-picture_show, staging, opera, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>sick life death bob flanagan supermasochist fe...</td>\n",
       "      <td>[sick, life, death, bob, flanagan, supermasoch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>everybody photographic_film think alicia docum...</td>\n",
       "      <td>[everybody, photographic_film, think, alicia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>lisa cholodenko high art intelligent quiet dra...</td>\n",
       "      <td>[lisa, cholodenko, high, art, intelligent, qui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  harmless silly fun comedy dim witte wrestle fa...   \n",
       "1      0  spot motion-picture_show staging opera go comp...   \n",
       "2      1  sick life death bob flanagan supermasochist fe...   \n",
       "3      0  everybody photographic_film think alicia docum...   \n",
       "4      1  lisa cholodenko high art intelligent quiet dra...   \n",
       "\n",
       "                                         word_tokens  \n",
       "0  [harmless, silly, fun, comedy, dim, witte, wre...  \n",
       "1  [spot, motion-picture_show, staging, opera, go...  \n",
       "2  [sick, life, death, bob, flanagan, supermasoch...  \n",
       "3  [everybody, photographic_film, think, alicia, ...  \n",
       "4  [lisa, cholodenko, high, art, intelligent, qui...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df.head():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>you've got mail works alot better than it dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>\" jaws \" is a rare film that grabs your atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  films adapted from comic books have had plenty...\n",
       "1      1  every now and then a movie comes along from a ...\n",
       "2      1  you've got mail works alot better than it dese...\n",
       "3      1   \" jaws \" is a rare film that grabs your atten...\n",
       "4      1  moviemaking is a lot like being the general ma..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"augmented_df.head():\")\n",
    "display(augmented_df.head())\n",
    "\n",
    "print(\"\\ndf.head():\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6fe295e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented_df.tail():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0</td>\n",
       "      <td>understand clich hell earth truly mean recentl...</td>\n",
       "      <td>[understand, clich, hell, earth, truly, mean, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0</td>\n",
       "      <td>1954 japanese monster film godzilla transform ...</td>\n",
       "      <td>[1954, japanese, monster, film, godzilla, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1</td>\n",
       "      <td>verdict spine chilling drama horror maestro st...</td>\n",
       "      <td>[verdict, spine, chilling, drama, horror, maes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0</td>\n",
       "      <td>midway Eunectes_murinus documentary film_maker...</td>\n",
       "      <td>[midway, Eunectes_murinus, documentary, film_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0</td>\n",
       "      <td>starship trooper bad movie mean bad movie cros...</td>\n",
       "      <td>[starship, trooper, bad, movie, mean, bad, mov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "3995      0  understand clich hell earth truly mean recentl...   \n",
       "3996      0  1954 japanese monster film godzilla transform ...   \n",
       "3997      1  verdict spine chilling drama horror maestro st...   \n",
       "3998      0  midway Eunectes_murinus documentary film_maker...   \n",
       "3999      0  starship trooper bad movie mean bad movie cros...   \n",
       "\n",
       "                                            word_tokens  \n",
       "3995  [understand, clich, hell, earth, truly, mean, ...  \n",
       "3996  [1954, japanese, monster, film, godzilla, tran...  \n",
       "3997  [verdict, spine, chilling, drama, horror, maes...  \n",
       "3998  [midway, Eunectes_murinus, documentary, film_m...  \n",
       "3999  [starship, trooper, bad, movie, mean, bad, mov...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df.tail():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>if anything , \" stigmata \" should be taken as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>john boorman's \" zardoz \" is a goofy cinematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>the kids in the hall are an acquired taste . \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>two party guys bob their heads to haddaway's d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1995      0  if anything , \" stigmata \" should be taken as ...\n",
       "1996      0  john boorman's \" zardoz \" is a goofy cinematic...\n",
       "1997      0  the kids in the hall are an acquired taste . \\...\n",
       "1998      0  there was a time when john carpenter was a gre...\n",
       "1999      0  two party guys bob their heads to haddaway's d..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"augmented_df.tail():\")\n",
    "display(augmented_df.tail())\n",
    "\n",
    "print(\"\\ndf.tail():\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ea632351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   2000 non-null   int64 \n",
      " 1   text    2000 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "augmented_df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   label        4000 non-null   int64 \n",
      " 1   text         4000 non-null   object\n",
      " 2   word_tokens  4000 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 93.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\ndf info:\")\n",
    "display(df.info())\n",
    "\n",
    "print(\"\\naugmented_df info:\")\n",
    "display(augmented_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "039b47bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df dimentions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "augmented_df dimentions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\ndf dimentions:\")\n",
    "display(df.shape)\n",
    "\n",
    "print(\"\\naugmented_df dimentions:\")\n",
    "display(augmented_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67965868",
   "metadata": {},
   "source": [
    "# Modelling - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a699ea3",
   "metadata": {},
   "source": [
    "####        â€¢    ML -> Logistic Regression, Naive Bayes, SVM, Decision tree, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d3f41270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, train_labels, feature_vector_test, test_labels, is_neural_net=False):\n",
    "    classifier.fit(feature_vector_train, train_labels)\n",
    "\n",
    "    train_predictions = classifier.predict(feature_vector_train)\n",
    "    test_predictions = classifier.predict(feature_vector_test)\n",
    "    train_accuracy = metrics.accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = metrics.accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return test_accuracy  # Return the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9d515",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2711bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 79.94%\n",
      "Test Accuracy: 78.00%\n"
     ]
    }
   ],
   "source": [
    "model_l1 = linear_model.LogisticRegression(penalty='l1', C=0.7, solver='liblinear', random_state=42)\n",
    "model_l1.fit(xtrain_tfidf, train_labels)\n",
    "\n",
    "accuracy = train_model(model_l1, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename_l1 = r\"Models\\LR_L1.joblib\"\n",
    "joblib.dump(model_l1, filename_l1)\n",
    "\n",
    "with open(r\"Accuracies\\LR_L1.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3128da47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 97.47%\n",
      "Test Accuracy: 91.75%\n"
     ]
    }
   ],
   "source": [
    "model_l2 = linear_model.LogisticRegression(penalty='l2', C=0.7, solver='liblinear', random_state=42)\n",
    "model_l2.fit(xtrain_tfidf, train_labels)\n",
    "\n",
    "accuracy = train_model(model_l2, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename_l2 = r\"Models\\LR_L2.joblib\"\n",
    "joblib.dump(model_l2, filename_l2)\n",
    "\n",
    "with open(r\"Accuracies\\LR_L2.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00af1b",
   "metadata": {},
   "source": [
    "#### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f6801ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 97.19%\n",
      "Test Accuracy: 91.25%\n"
     ]
    }
   ],
   "source": [
    "model = naive_bayes.MultinomialNB(alpha=0.7)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\NB.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\NB.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1b670",
   "metadata": {},
   "source": [
    "#### 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1e74a87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 96.22%\n",
      "Test Accuracy: 91.25%\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC(kernel='linear', C=0.3)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\SVM_Linear.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\SVM_Linear.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4f9d0071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.41%\n",
      "Test Accuracy: 91.75%\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC(kernel='rbf', C=0.3)\n",
    "model.fit(xtrain_tfidf, train_labels)\n",
    "\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "joblib.dump(model, r\"Models\\SVM_rbf.joblib\")\n",
    "\n",
    "with open(r\"Accuracies\\SVM_rbf.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6a3eefc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 83.38%\n",
      "Test Accuracy: 50.38%\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC(kernel='poly', C=0.3, degree=4)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\SVM_Poly.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\SVM_Poly.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e209c",
   "metadata": {},
   "source": [
    "#### 4. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "067e770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 84.50%\n",
      "Test Accuracy: 74.88%\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=3, random_state=42)\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\DT.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\DT.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba9625",
   "metadata": {},
   "source": [
    "#### 5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7281ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 96.03%\n",
      "Test Accuracy: 87.12%\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "accuracy = train_model(model, xtrain_tfidf, train_labels, xtest_tfidf, test_labels)\n",
    "\n",
    "filename = \"Models\\\\RF.joblib\"\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "with open(\"Accuracies\\\\RF.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe38d91",
   "metadata": {},
   "source": [
    "#### â€¢ DL -> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4ba3d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Use same train/test split for BERT\n",
    "# bert_train_texts = train_texts.tolist()\n",
    "# bert_test_texts = test_texts.tolist()\n",
    "# bert_train_labels = train_labels.tolist()\n",
    "# bert_test_labels = test_labels.tolist()\n",
    "\n",
    "# bert_train_ds = Dataset.from_dict({\"text\": bert_train_texts, \"label\": bert_train_labels})\n",
    "# bert_test_ds = Dataset.from_dict({\"text\": bert_test_texts, \"label\": bert_test_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "21bd2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Tokenize the datasets\n",
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# bert_train_ds = bert_train_ds.map(tokenize_function, batched=True)\n",
    "# bert_test_ds = bert_test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Set PyTorch format\n",
    "# bert_train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# bert_test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "addefa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load pre-trained BERT for classification\n",
    "# from transformers import DistilBertForSequenceClassification\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# # Define evaluation metric\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "#     return {\"accuracy\": accuracy_score(labels, predictions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f8aadf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"no\",\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=bert_train_ds,\n",
    "#     eval_dataset=bert_test_ds,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "626ffc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate BERT model on test set\n",
    "# metrics = trainer.evaluate()\n",
    "# print(f\"BERT Classification Accuracy: {metrics['eval_accuracy']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
